# SchemaForge ğŸ”¨

<div align="center">

**Transform JSON Chaos into Analytics-Ready Data**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Code Style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

[Quick Start](#-quick-start) â€¢ [Features](#-features) â€¢ [Documentation](#-documentation) â€¢ [CLI Reference](#-cli-reference)

</div>

---

## ğŸ¯ What is SchemaForge?

SchemaForge automatically discovers JSON schemas and converts them to analytics-ready formats. **Stop wasting hours on manual data wrangling**â€”let SchemaForge handle type detection, schema inference, and format conversion in seconds.

### The Problem vs. The Solution

<table>
<tr>
<td width="50%">

**âŒ Traditional Workflow**
```
ğŸ“„ JSON Files
    â†“ (manual analysis)
ğŸ“ Write Schemas
    â†“ (write conversion code)
ğŸ› Debug Type Errors
    â†“ (fix, repeat)
â° Hours Later...
    â†“
âœ… Ready for Analysis
```

</td>
<td width="50%">

**âœ… SchemaForge Workflow**
```
ğŸ“„ JSON Files
    â†“ (one command)
ğŸ” Auto Schema Discovery
    â†“ (one command)
âœ… Parquet/CSV/Avro/ORC
    â†“
âš¡ Minutes Later!
```

</td>
</tr>
</table>

**Time Saved:** Hours â†’ Minutes | **Errors:** Many â†’ Zero

---

## âœ¨ Features

### ğŸ§  Intelligent Schema Discovery
- **Advanced Type Detection** - Strings, numbers, booleans, timestamps, URLs, emails, UUIDs, IPs, and more
- **Smart Pattern Recognition** - Automatically detects enums, embedded JSON, and numeric strings
- **Statistical Analysis** - Collects min/max, length stats, and value distributions
- **Nested Structure Handling** - Flattens complex JSON with intuitive dot notation

### ğŸ“ Universal Format Support
**Input:** 11+ JSON formats auto-detected
- Standard JSON Arrays â€¢ NDJSON â€¢ Wrapper Objects â€¢ GeoJSON â€¢ Socrata/OpenData â€¢ Single Objects â€¢ Python Literals â€¢ Embedded JSON

**Output:** 4 analytics-ready formats
- **Parquet** (recommended) â€¢ **CSV** â€¢ **Avro** â€¢ **ORC**

### ğŸš€ Production-Ready Tools
- **Schema Validation** - Verify data quality before processing
- **Performance Benchmarking** - Measure and optimize your pipelines
- **Batch Processing** - Convert multiple files in one command
- **Sampling Support** - Handle massive files efficiently

---

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/Syntax-Error-1337/SchemaForge.git
cd SchemaForge

# Install dependencies
pip install -r requirements.txt
```

**Requirements:** Python 3.8+, pandas, pyarrow, fastavro, ijson

---

## ğŸš€ Quick Start

### Three Simple Steps

```bash
# 1ï¸âƒ£ Place your JSON files in the data directory
cp your_data/*.json data/

# 2ï¸âƒ£ Discover schemas
python -m src.cli scan-schemas

# 3ï¸âƒ£ Convert to your preferred format
python -m src.cli convert --format parquet
```

**That's it!** Your data is now in `output/`, ready for analysis. ğŸ‰

### What Just Happened?

- âœ… All JSON structures automatically analyzed
- âœ… Types inferred with statistical confidence
- âœ… Nested objects flattened intelligently
- âœ… Schema reports generated (Markdown + JSON)
- âœ… Data converted to optimized format

---

## ğŸ”§ CLI Reference

### Core Commands

| Command | Purpose | Example |
|---------|---------|---------|
| `scan-schemas` | Analyze JSON structure | `python -m src.cli scan-schemas` |
| `convert` | Transform to analytics formats | `python -m src.cli convert --format parquet` |
| `validate` | Verify schema compliance | `python -m src.cli validate` |
| `benchmark` | Measure performance | `python -m src.cli benchmark` |

### `scan-schemas` - Discover JSON Schemas

```bash
python -m src.cli scan-schemas [OPTIONS]
```

**Options:**
- `--data-dir` - Input directory (default: `data`)
- `--output-report` - Report path (default: `reports/schema_report.md`)
- `--max-sample-size` - Sample size for large files
- `--sampling-strategy` - `first` or `random` sampling

**Examples:**
```bash
# Basic usage
python -m src.cli scan-schemas

# Large files with random sampling
python -m src.cli scan-schemas --max-sample-size 10000 --sampling-strategy random

# Custom directory
python -m src.cli scan-schemas --data-dir my_json_data --output-report custom/schema.md
```

**Output:**
- `schema_report.md` - Human-readable documentation
- `schema_report.json` - Machine-readable schema

---

### `convert` - Transform to Analytics Formats

```bash
python -m src.cli convert --format [parquet|csv|avro|orc] [OPTIONS]
```

**Options:**
- `--format` - **Required:** Output format
- `--data-dir` - Input directory (default: `data`)
- `--output-dir` - Output directory (default: `output`)
- `--schema-report` - Schema JSON path (default: `reports/schema_report.json`)

**Examples:**
```bash
# Convert to Parquet (recommended for big data)
python -m src.cli convert --format parquet

# Convert to CSV (universal compatibility)
python -m src.cli convert --format csv

# Convert to Avro (schema evolution)
python -m src.cli convert --format avro

# Custom directories
python -m src.cli convert --format parquet --data-dir raw_data --output-dir lake/
```

> **âš ï¸ Note:** Run `scan-schemas` first to generate the schema report.

---

### `validate` - Verify Data Quality

```bash
python -m src.cli validate [OPTIONS]
```

**Options:**
- `--data-dir` - Directory to validate (default: `data`)
- `--schema-report` - Schema for validation (default: `reports/schema_report.json`)

**Example:**
```bash
python -m src.cli validate --data-dir production_data
```

---

### `benchmark` - Performance Testing

```bash
python -m src.cli benchmark [OPTIONS]
```

**Options:**
- `--type` - Benchmark type: `schema`, `conversion`, or `all` (default: `all`)
- `--formats` - Formats to test (default: `parquet,csv,avro,orc`)
- `--result-dir` - Results directory (default: `result`)

**Example:**
```bash
python -m src.cli benchmark --type all --result-dir benchmarks/
```

---

## ğŸ’¼ Use Cases

### ğŸ¢ Data Engineering
**Challenge:** Inconsistent JSON from multiple APIs  
**Solution:** Unified schema discovery and conversion  
**Result:** 80% faster pipeline development

### ğŸ”¬ Research Data
**Challenge:** Diverse datasets from experiments and surveys  
**Solution:** One-command conversion to analysis-ready formats  
**Result:** More time analyzing, less time wrangling

### ğŸŒ Open Data
**Challenge:** Complex formats from Socrata/CKAN portals  
**Solution:** Automatic column extraction and transformation  
**Result:** Easy access to government datasets

### ğŸ—„ï¸ Data Lakes
**Challenge:** Efficient storage for massive JSON collections  
**Solution:** Convert to optimized columnar formats  
**Result:** Better compression, faster queries, lower costs

---

## ğŸ“– Documentation

### Supported JSON Formats

SchemaForge automatically detects and handles:

1. **Standard JSON Array** - `[{...}, {...}]`
2. **NDJSON** - Newline-delimited records
3. **Wrapper Objects** - `{"data": [...], "meta": {...}}`
4. **Socrata/OpenData** - Array-based tabular format
5. **GeoJSON** - Geographic feature collections
6. **Single Objects** - Individual JSON records
7. **Embedded JSON** - JSON strings within fields

### Schema Inference

**Detected Types:**
- Basic: `string`, `integer`, `float`, `boolean`
- Advanced: `timestamp`, `url`, `email`, `uuid`, `ip_address`
- Structured: `array<T>`, `object`, `json_string`
- Special: `numeric_string`, `enum`

**Features:**
- âœ… Nested structure flattening (`user.address.city`)
- âœ… Nullable field detection
- âœ… Mixed type recognition
- âœ… Statistical profiling (min/max, length, distributions)
- âœ… Enum detection for categorical fields

### Complete Workflow Example

```bash
# 1. Scan and analyze
python -m src.cli scan-schemas --data-dir raw_data

# 2. Validate quality
python -m src.cli validate --data-dir raw_data

# 3. Convert for analytics
python -m src.cli convert --format parquet --output-dir processed/

# 4. Benchmark performance
python -m src.cli benchmark --type all
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run specific test
pytest tests/test_schema_reader.py
```

**Test Coverage:**
- âœ… All 11+ JSON formats
- âœ… Type inference for all data types
- âœ… Format conversion (Parquet/CSV/Avro/ORC)
- âœ… Error handling and edge cases

---

## ğŸ¯ Performance Tips

1. **Use Sampling for Large Files**
   ```bash
   python -m src.cli scan-schemas --max-sample-size 10000 --sampling-strategy random
   ```

2. **Choose the Right Format**
   - **Parquet** â†’ Big data analytics (best compression)
   - **Avro** â†’ Schema evolution & streaming
   - **ORC** â†’ Hadoop/Hive ecosystems
   - **CSV** â†’ Universal compatibility

3. **Monitor Performance**
   ```bash
   python -m src.cli benchmark --type all
   ```

See [BENCHMARK_OPTIMIZATION.md](BENCHMARK_OPTIMIZATION.md) for detailed optimization guide.

---

## ğŸ—ï¸ Project Structure

```
SchemaForge/
â”œâ”€â”€ data/              # Input JSON files
â”œâ”€â”€ output/            # Converted files
â”œâ”€â”€ reports/           # Schema reports (.md + .json)
â”œâ”€â”€ result/            # Benchmark results
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ schema_reader.py    # Schema inference engine
â”‚   â”œâ”€â”€ converter.py        # Format conversion
â”‚   â”œâ”€â”€ json_loader.py      # JSON format detection
â”‚   â”œâ”€â”€ validator.py        # Schema validation
â”‚   â”œâ”€â”€ benchmark.py        # Performance testing
â”‚   â””â”€â”€ cli.py              # Command-line interface
â””â”€â”€ tests/             # Test suite
```

---

## ğŸ¤ Contributing

We welcome contributions! Here's how:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing`)
3. Make your changes with tests
4. Run tests (`pytest tests/ -v`)
5. Submit a Pull Request

**Ideas for Contributions:**
- Schema versioning tools
- Streaming processing for huge files
- GUI/Web interface
- Database export support
- Additional output formats

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

Built for data engineers, researchers, and developers who are tired of manual schema definitions.

**Powered by:** pandas â€¢ pyarrow â€¢ fastavro â€¢ ijson â€¢ pytest

---

## ğŸ“ Support

- **Issues:** [GitHub Issues](https://github.com/Syntax-Error-1337/SchemaForge/issues)
- **Discussions:** [GitHub Discussions](https://github.com/Syntax-Error-1337/SchemaForge/discussions)
- **Documentation:** This README

**Before opening an issue:**
1. Check existing issues
2. Try `python -m src.cli [command] --help`
3. Run `pytest tests/ -v` to verify installation

---

<div align="center">

**SchemaForge** - Transform Data Chaos into Analytics Gold ğŸ”¨

â­ **Star us on GitHub if SchemaForge saved you time!** â­

[â¬† Back to Top](#schemaforge-)

</div>
