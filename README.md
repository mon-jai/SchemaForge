# SchemaForge ğŸ”¨

<div align="center">

![SchemaForge Logo](https://img.shields.io/badge/SchemaForge-Data%20Intelligence-orange?style=for-the-badge&logo=databricks&logoColor=white)

**Intelligent JSON Schema Discovery & Data Transformation**

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)](tests/)

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Quick Start](#-quick-start) â€¢ [Documentation](#-documentation) â€¢ [Use Cases](#-use-cases)

</div>

---

## ğŸ¯ What is SchemaForge?

SchemaForge is a **schema-first data pipeline tool** that automatically discovers JSON structures and converts them to analytics-ready formats. Stop wasting time on manual schema definitions and data wranglingâ€”let SchemaForge do the heavy lifting.

### Why SchemaForge?

```
Traditional Workflow:          SchemaForge Workflow:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“„ JSON Files                  ğŸ“„ JSON Files
    â†“                              â†“
âš™ï¸  Manual Analysis            ğŸ” Automatic Scan
    â†“                              â†“
ğŸ“ Write Schemas               ğŸ“Š Schema Report
    â†“                              â†“
ğŸ’» Write Code                  ğŸ”¨ One Command
    â†“                              â†“
ğŸ› Debug Type Errors           âœ… Parquet/CSV
    â†“
â° Hours Later...
    â†“
âœ… Parquet/CSV

Time: Hours â†’ Minutes
Errors: Many â†’ Zero
```

---

## âœ¨ Features

### ğŸ§  Intelligent Schema Inference
- **Automatic Type Detection**: Strings, integers, floats, booleans, timestamps, arrays, objects
- **Nested Structure Handling**: Flattens nested JSON with dot notation (`user.address.city`)
- **Nullable Field Detection**: Identifies which fields can be null
- **Mixed Type Recognition**: Detects and reports inconsistent types across records

### ğŸ“ Multi-Format JSON Support
Handles 6+ JSON formats automatically:
- âœ… **Standard JSON Arrays**: `[{...}, {...}]`
- âœ… **NDJSON** (Newline-Delimited): One object per line
- âœ… **Wrapper Objects**: `{data: [...]}`, `{results: [...]}`, etc.
- âœ… **Array-Based Tabular**: Socrata/OpenData format with metadata
- âœ… **GeoJSON**: FeatureCollection format
- âœ… **Single Objects**: Single-record datasets

### ğŸ”„ Schema-First Workflow
1. **Scan once** â†’ Generate comprehensive schema reports
2. **Review** â†’ Human-readable Markdown + machine-readable JSON
3. **Convert everywhere** â†’ Consistent schemas across all conversions

### ğŸš€ Production-Ready
- **Robust Error Handling**: Graceful failures, detailed logging
- **Sampling Support**: Process large files efficiently
- **Batch Processing**: Convert multiple files in one command
- **Type Coercion**: Intelligent type conversion with fallbacks

### ğŸ“Š Dual Report Format
- **Markdown Report**: Beautiful, human-readable documentation
- **JSON Report**: Machine-readable schema for programmatic use

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Install Dependencies

```bash
# Clone the repository
git clone https://github.com/yourusername/schemaforge.git
cd schemaforge

# Install required packages
pip install -r requirements.txt
```

### Required Packages
```
pandas>=2.0.0      # Data manipulation
pyarrow>=12.0.0    # Parquet support
pytest>=7.0.0      # Testing framework
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Place Your JSON Files
```bash
# Copy your JSON files to the data directory
cp your_data/*.json data/
```

### 2ï¸âƒ£ Discover Schemas
```bash
# Scan all JSON files and generate schema reports
python -m src.cli scan-schemas
```

**Output:**
- `reports/schema_report.md` - Beautiful, human-readable report
- `reports/schema_report.json` - Machine-readable schema definitions

### 3ï¸âƒ£ Review the Schema
```bash
# Check the generated report
cat reports/schema_report.md
```

### 4ï¸âƒ£ Convert to Parquet or CSV
```bash
# Convert to Parquet (recommended for analytics)
python -m src.cli convert --format parquet

# Or convert to CSV
python -m src.cli convert --format csv
```

**That's it!** Your data is now in `output/` directory, ready for analysis.

---

## ğŸ“– Documentation

### Command Reference

#### `scan-schemas` - Discover JSON Schemas

```bash
python -m src.cli scan-schemas [OPTIONS]
```

**Options:**
| Option | Description | Default |
|--------|-------------|---------|
| `--data-dir` | Input directory containing JSON files | `data` |
| `--output-report` | Path for Markdown report | `reports/schema_report.md` |
| `--max-sample-size` | Max records to analyze per file | All records |
| `--sampling-strategy` | Sampling method: `first` or `random` | `first` |

**Examples:**
```bash
# Basic usage
python -m src.cli scan-schemas

# Analyze only first 1000 records per file
python -m src.cli scan-schemas --max-sample-size 1000

# Use random sampling for better representation
python -m src.cli scan-schemas --sampling-strategy random --max-sample-size 500

# Custom data directory
python -m src.cli scan-schemas --data-dir my_json_data
```

---

#### `convert` - Transform JSON to Parquet/CSV

```bash
python -m src.cli convert --format [parquet|csv] [OPTIONS]
```

**Options:**
| Option | Description | Default |
|--------|-------------|---------|
| `--format` | Output format: `parquet` or `csv` | **Required** |
| `--data-dir` | Input directory | `data` |
| `--output-dir` | Output directory | `output` |
| `--schema-report` | JSON schema report path | `reports/schema_report.json` |

**Examples:**
```bash
# Convert to Parquet
python -m src.cli convert --format parquet

# Convert to CSV with custom directories
python -m src.cli convert --format csv \
  --data-dir my_data \
  --output-dir csv_output

# Use custom schema report
python -m src.cli convert --format parquet \
  --schema-report custom_schemas/report.json
```

---

### Supported JSON Formats

<details>
<summary><b>1ï¸âƒ£ Standard JSON Array</b></summary>

```json
[
  {"id": 1, "name": "Alice", "age": 30},
  {"id": 2, "name": "Bob", "age": 25}
]
```
**Use case:** Most common JSON format from APIs and exports
</details>

<details>
<summary><b>2ï¸âƒ£ Newline-Delimited JSON (NDJSON)</b></summary>

```
{"id": 1, "name": "Alice", "age": 30}
{"id": 2, "name": "Bob", "age": 25}
```
**Use case:** Log files, streaming data, large datasets
</details>

<details>
<summary><b>3ï¸âƒ£ Wrapper Objects</b></summary>

```json
{
  "data": [
    {"id": 1, "name": "Alice"},
    {"id": 2, "name": "Bob"}
  ],
  "metadata": {...}
}
```
**Auto-detected fields:** `data`, `results`, `items`, `records`, `rows`, `entries`

**Use case:** API responses with metadata
</details>

<details>
<summary><b>4ï¸âƒ£ Array-Based Tabular Data</b></summary>

```json
{
  "meta": {
    "view": {
      "columns": [
        {"name": "id", "fieldName": "id", "dataTypeName": "number"},
        {"name": "name", "fieldName": "name", "dataTypeName": "text"}
      ]
    }
  },
  "data": [
    [1, "Alice"],
    [2, "Bob"]
  ]
}
```
**Use case:** Socrata, CKAN, and other open data portals

**Features:**
- âœ… Extracts column definitions from metadata
- âœ… Converts arrays to objects using column names
- âœ… Skips hidden/meta columns automatically
</details>

<details>
<summary><b>5ï¸âƒ£ GeoJSON Format</b></summary>

```json
{
  "type": "FeatureCollection",
  "features": [
    {
      "type": "Feature",
      "properties": {"name": "Location 1", "value": 100},
      "geometry": {"type": "Point", "coordinates": [-122.4, 37.8]}
    }
  ]
}
```
**Use case:** Geographic data from mapping APIs

**Note:** Extracts `properties` field from features
</details>

<details>
<summary><b>6ï¸âƒ£ Single JSON Object</b></summary>

```json
{
  "id": 1,
  "name": "Alice",
  "address": {
    "city": "New York",
    "zip": "10001"
  }
}
```
**Use case:** Configuration files, single-record exports

**Note:** Treated as a single-record dataset
</details>

---

### Schema Inference Rules

#### Data Types
SchemaForge detects these types automatically:

| Type | Description | Example |
|------|-------------|---------|
| `string` | Text data | `"Alice"` |
| `integer` | Whole numbers | `42` |
| `float` | Decimal numbers | `3.14` |
| `boolean` | True/false | `true` |
| `timestamp` | Date/time strings | `"2023-01-01T10:00:00Z"` |
| `array<T>` | Lists of values | `["a", "b", "c"]` |
| `object` | Nested structures | `{"key": "value"}` |

#### Nested Structures
Nested objects are flattened with dot notation:

**Input:**
```json
{
  "user": {
    "name": "Alice",
    "address": {
      "city": "NYC"
    }
  }
}
```

**Output Columns:**
- `user.name` (string)
- `user.address.city` (string)

#### Nullable Fields
Fields containing `null` values are marked as nullable in the schema.

---

## ğŸ’¼ Use Cases

### ğŸ¢ Data Engineering & ETL Pipelines
**Problem:** Building data pipelines with inconsistent JSON from multiple sources  
**Solution:** Automatic schema discovery and Parquet conversion  
**Benefit:** 80% faster pipeline development, consistent data types

```bash
# Example workflow
python -m src.cli scan-schemas --data-dir api_exports/
python -m src.cli convert --format parquet --output-dir data_lake/
```

---

### ğŸ”¬ Research Data Processing
**Problem:** Diverse JSON datasets from experiments, surveys, APIs  
**Solution:** One-command conversion to analysis-ready formats  
**Benefit:** More time for research, less time on data wrangling

**Example use cases:**
- Social media data analysis
- Scientific instrument outputs
- Survey response processing
- Open data portal research

---

### ğŸŒ Open Data Portal Integration
**Problem:** Socrata/CKAN array-based format is difficult to work with  
**Solution:** Automatic column extraction and conversion  
**Benefit:** Easy access to government and public datasets

**Supported portals:**
- data.gov datasets
- City open data portals
- Research institution repositories

---

### ğŸ”„ API Data Integration
**Problem:** REST APIs return JSON in various formats  
**Solution:** Schema-first approach ensures consistency  
**Benefit:** Reliable data integration into warehouses

---

### ğŸ—„ï¸ Data Lake Ingestion
**Problem:** Need efficient storage format for JSON in data lakes  
**Solution:** Convert to Parquet with preserved schemas  
**Benefit:** Better compression, faster queries, lower costs

---

### ğŸ”„ Data Migration & Format Conversion
**Problem:** Migrating from JSON-based systems to columnar formats  
**Solution:** Intelligent schema inference preserves data semantics  
**Benefit:** Accurate migrations without data loss

---

## ğŸ—ï¸ Project Structure

```
schemaforge/
â”œâ”€â”€ ğŸ“ data/                    # Input JSON files (place your data here)
â”‚   â””â”€â”€ *.json                 # Your JSON data files
â”œâ”€â”€ ğŸ“ output/                  # Converted output files
â”‚   â”œâ”€â”€ *.parquet              # Parquet output files
â”‚   â””â”€â”€ *.csv                  # CSV output files
â”œâ”€â”€ ğŸ“ reports/                 # Generated schema reports
â”‚   â”œâ”€â”€ schema_report.md       # Human-readable report
â”‚   â””â”€â”€ schema_report.json     # Machine-readable report
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ schema_reader.py       # Schema inference engine
â”‚   â”œâ”€â”€ converter.py           # Format conversion module
â”‚   â””â”€â”€ cli.py                 # Command-line interface
â”œâ”€â”€ ğŸ“ tests/
â”‚   â”œâ”€â”€ test_schema_reader.py
â”‚   â””â”€â”€ test_converter.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ pytest.ini
```

---

## ğŸ”§ Advanced Usage

### Large File Processing

For very large JSON files, use sampling:

```bash
# Analyze first 10,000 records only
python -m src.cli scan-schemas --max-sample-size 10000

# Random sample for better representation
python -m src.cli scan-schemas \
  --sampling-strategy random \
  --max-sample-size 5000
```

### Programmatic Usage

Use SchemaForge as a Python library:

```python
from src.schema_reader import SchemaReader
from src.converter import Converter
from pathlib import Path

# Discover schemas
reader = SchemaReader(
    data_dir=Path("data"),
    max_sample_size=1000
)
schemas = reader.scan_directory()
reader.generate_report(schemas, output_path=Path("reports/schema.md"))

# Convert with schema
converter = Converter(
    data_dir=Path("data"),
    output_dir=Path("output")
)
converter.convert_all(format="parquet", schema_report_path=Path("reports/schema.json"))
```

### Custom Type Handling

Extend type inference for custom formats:

```python
from src.schema_reader import SchemaReader

class CustomSchemaReader(SchemaReader):
    def _infer_type(self, value):
        # Add custom type detection
        if isinstance(value, str) and value.startswith("http"):
            return "url"
        return super()._infer_type(value)
```

---

## ğŸ§ª Testing

Run the full test suite:

```bash
# Run all tests
pytest tests/

# Run with verbose output
pytest tests/ -v

# Run specific test file
pytest tests/test_schema_reader.py

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

---

## ğŸ¯ Architecture

### Two-Phase Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Phase 1: Schema Discovery              â”‚
â”‚                                                       â”‚
â”‚  JSON Files â†’ Format Detection â†’ Schema Inference   â”‚
â”‚     â†“              â†“                   â†“             â”‚
â”‚  Load Data â†’ Extract Columns â†’ Analyze Types        â”‚
â”‚                                    â†“                 â”‚
â”‚                          Generate Reports            â”‚
â”‚                       (Markdown + JSON)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Phase 2: Conversion                  â”‚
â”‚                                                       â”‚
â”‚  Schema Report â†’ Load Data â†’ Apply Schema           â”‚
â”‚       â†“             â†“            â†“                   â”‚
â”‚  Type Coercion â†’ Flatten â†’ Convert Format           â”‚
â”‚                               â†“                      â”‚
â”‚                    Parquet/CSV Output                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLI Layer   â”‚  â† User commands (scan-schemas, convert)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Schema Reader â”‚  â”‚  Converter   â”‚  â”‚JSON Loader   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚                 â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  JSON Files  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš¨ Known Limitations

| Limitation | Description | Workaround |
|------------|-------------|------------|
| **Memory Usage** | Large files loaded into memory | Use `--max-sample-size` for schema inference |
| **Array of Objects** | Stored as JSON strings in output | Design choice for flat file compatibility |
| **Type Coercion** | Best-effort conversion | Manual validation recommended |
| **Timestamp Detection** | Pattern-based recognition | May miss custom formats |
| **Encoding** | Assumes UTF-8 | Convert files to UTF-8 first |

---

## ğŸ¤ Contributing

We welcome contributions! Here are some ideas:

### Features to Add
- [ ] Avro and ORC output formats
- [ ] Schema validation against inferred schemas
- [ ] Streaming processing for very large files
- [ ] Schema versioning and migration tools
- [ ] Database export capabilities (PostgreSQL, MySQL)
- [ ] GUI/Web interface
- [ ] Docker support
- [ ] Schema diff tool

### How to Contribute
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

Built with love for:
- Data engineers struggling with inconsistent JSON
- Researchers drowning in data wrangling
- Developers tired of manual schema definitions
- Anyone who's ever said "I wish this JSON had a schema"

---

## ğŸ“ Support

- **Documentation**: [Full documentation](#-documentation)
- **Issues**: [GitHub Issues](https://github.com/yourusername/schemaforge/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/schemaforge/discussions)

---

## ğŸŒŸ Star History

If SchemaForge saved you time, consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=yourusername/schemaforge&type=Date)](https://star-history.com/#Syntax-Error-1337/schemaforge&Date)

---

<div align="center">

**Made with** ğŸ”¨ **by developers, for developers**

[â¬† Back to Top](#schemaforge-)

</div>
